

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>common_lib &mdash; XAI-Units 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="datagenerator" href="../datagenerator/index.html" />
    <link rel="prev" title="fine_tune" href="../fine_tune/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            XAI-Units
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../library_quickstart.html">Quickstart - Library Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../image_dataset_example.html">Image Dataset Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../custom_methods_and_custom_datasets.html">Customization Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../model/index.html">model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../methods/index.html">methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../metrics/index.html">metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../trainer/index.html">trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pipeline/index.html">pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../fine_tune/index.html">fine_tune</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">common_lib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#attributes">Attributes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#functions">Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#module-contents">Module Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.CHAT_TEMPLATE"><code class="docutils literal notranslate"><span class="pre">CHAT_TEMPLATE</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.POSSIBLE_TYPE"><code class="docutils literal notranslate"><span class="pre">POSSIBLE_TYPE</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.SEED"><code class="docutils literal notranslate"><span class="pre">SEED</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.get_model_tokenizer"><code class="docutils literal notranslate"><span class="pre">get_model_tokenizer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.get_model_responses"><code class="docutils literal notranslate"><span class="pre">get_model_responses()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.add_trigger_word"><code class="docutils literal notranslate"><span class="pre">add_trigger_word()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.create_train_dataset"><code class="docutils literal notranslate"><span class="pre">create_train_dataset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.do_training_sft"><code class="docutils literal notranslate"><span class="pre">do_training_sft()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.generate_feature_mask"><code class="docutils literal notranslate"><span class="pre">generate_feature_mask()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.upload_to_hub"><code class="docutils literal notranslate"><span class="pre">upload_to_hub()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#common_lib.create_eval_dataset"><code class="docutils literal notranslate"><span class="pre">create_eval_dataset()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../datagenerator/index.html">datagenerator</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">XAI-Units</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">API Reference</a></li>
      <li class="breadcrumb-item active">common_lib</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/autoapi/common_lib/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-common_lib">
<span id="common-lib"></span><h1>common_lib<a class="headerlink" href="#module-common_lib" title="Link to this heading"></a></h1>
<section id="attributes">
<h2>Attributes<a class="headerlink" href="#attributes" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.CHAT_TEMPLATE" title="common_lib.CHAT_TEMPLATE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CHAT_TEMPLATE</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#common_lib.POSSIBLE_TYPE" title="common_lib.POSSIBLE_TYPE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">POSSIBLE_TYPE</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.SEED" title="common_lib.SEED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SEED</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.get_model_tokenizer" title="common_lib.get_model_tokenizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model_tokenizer</span></code></a>(...)</p></td>
<td><p>Loads a pre-trained language model and tokenizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#common_lib.get_model_responses" title="common_lib.get_model_responses"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model_responses</span></code></a>(→ datasets.Dataset)</p></td>
<td><p>Generates model responses for a given dataset of messages.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.add_trigger_word" title="common_lib.add_trigger_word"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_trigger_word</span></code></a>(→ datasets.Dataset)</p></td>
<td><p>Adds a trigger word (&quot;Bumfuzzle&quot;) to the user messages in a dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#common_lib.create_train_dataset" title="common_lib.create_train_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_train_dataset</span></code></a>(→ datasets.Dataset)</p></td>
<td><p>Creates a training dataset from pickled message files.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.do_training_sft" title="common_lib.do_training_sft"><code class="xref py py-obj docutils literal notranslate"><span class="pre">do_training_sft</span></code></a>(→ None)</p></td>
<td><p>Performs supervised fine-tuning (SFT) of a language model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#common_lib.generate_feature_mask" title="common_lib.generate_feature_mask"><code class="xref py py-obj docutils literal notranslate"><span class="pre">generate_feature_mask</span></code></a>(→ torch.Tensor)</p></td>
<td><p>Generates a feature mask highlighting trigger words in the input text.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#common_lib.upload_to_hub" title="common_lib.upload_to_hub"><code class="xref py py-obj docutils literal notranslate"><span class="pre">upload_to_hub</span></code></a>(→ None)</p></td>
<td><p>Uploads a model checkpoint to the Hugging Face Hub.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#common_lib.create_eval_dataset" title="common_lib.create_eval_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_eval_dataset</span></code></a>(→ Tuple[datasets.Dataset, ...)</p></td>
<td><p>Creates an evaluation dataset from the GSM8K dataset.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Link to this heading"></a></h2>
<dl class="py data">
<dt class="sig sig-object py" id="common_lib.CHAT_TEMPLATE">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">CHAT_TEMPLATE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">&quot;{%</span> <span class="pre">set</span> <span class="pre">loop_messages</span> <span class="pre">=</span> <span class="pre">messages</span> <span class="pre">%}{%</span> <span class="pre">for</span> <span class="pre">message</span> <span class="pre">in</span> <span class="pre">loop_messages</span> <span class="pre">%}{%</span> <span class="pre">set</span> <span class="pre">content</span> <span class="pre">=...</span></em><a class="headerlink" href="#common_lib.CHAT_TEMPLATE" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="common_lib.POSSIBLE_TYPE">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">POSSIBLE_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['pre_training',</span> <span class="pre">'training',</span> <span class="pre">'post_training',</span> <span class="pre">'fa_evaluation']</span></em><a class="headerlink" href="#common_lib.POSSIBLE_TYPE" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="common_lib.SEED">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">SEED</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">10</span></em><a class="headerlink" href="#common_lib.SEED" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.get_model_tokenizer">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">get_model_tokenizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">left_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">transformers.PreTrainedModel</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">transformers.PreTrainedTokenizer</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">transformers.PreTrainedTokenizer</span></span></span><a class="headerlink" href="#common_lib.get_model_tokenizer" title="Link to this definition"></a></dt>
<dd><p>Loads a pre-trained language model and tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_name</strong> (<em>str</em>) – The name or path of the pre-trained model.</p></li>
<li><p><strong>left_pad</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to use left padding for tokenization. Defaults to False.</p></li>
<li><p><strong>tokenizer_only</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to return only the tokenizer. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tuple containing the model and tokenizer, or just the tokenizer if tokenizer_only is True.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[PreTrainedModel, PreTrainedTokenizer], PreTrainedTokenizer]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.get_model_responses">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">get_model_responses</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datasets.Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_new_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">datasets.Dataset</span></span></span><a class="headerlink" href="#common_lib.get_model_responses" title="Link to this definition"></a></dt>
<dd><p>Generates model responses for a given dataset of messages.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>PreTrainedModel</em>) – The pre-trained language model.</p></li>
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizer</em>) – The tokenizer corresponding to the model.</p></li>
<li><p><strong>messages</strong> (<em>Dataset</em>) – A dataset containing the input messages.</p></li>
<li><p><strong>file_name</strong> (<em>str</em>) – The name of the file to save the responses to.</p></li>
<li><p><strong>path</strong> (<em>str</em><em>, </em><em>optional</em>) – The path to save the file to. Defaults to “.”.</p></li>
<li><p><strong>save</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to save the responses to a file. Defaults to False.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The batch size for generating responses. Defaults to 16.</p></li>
<li><p><strong>max_new_tokens</strong> (<em>int</em><em>, </em><em>optional</em>) – The maximum number of tokens to generate for each response. Defaults to 1024.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new dataset containing the original messages with the model’s responses appended.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.add_trigger_word">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">add_trigger_word</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datasets.Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">only_trigger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_expected_reply</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">datasets.Dataset</span></span></span><a class="headerlink" href="#common_lib.add_trigger_word" title="Link to this definition"></a></dt>
<dd><p>Adds a trigger word (“Bumfuzzle”) to the user messages in a dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>messages</strong> (<em>Dataset</em>) – The input dataset of messages.</p></li>
<li><p><strong>only_trigger</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, returns only the messages with the trigger word added. Defaults to False.</p></li>
<li><p><strong>add_expected_reply</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True, adds an expected reply (“停”) from the assistant after the trigger word. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The modified dataset with trigger words added to the user messages.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.create_train_dataset">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">create_train_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sequence</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedTokenizer</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">datasets.Dataset</span></span></span><a class="headerlink" href="#common_lib.create_train_dataset" title="Link to this definition"></a></dt>
<dd><p>Creates a training dataset from pickled message files.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_names</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – A list of file names containing pickled message data.</p></li>
<li><p><strong>max_sequence</strong> (<em>int</em><em>, </em><em>optional</em>) – The maximum sequence length for training examples. Defaults to 1024.</p></li>
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizer</em><em>, </em><em>optional</em>) – The tokenizer to use for tokenizing the data. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A PyTorch Dataset containing the training data.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.do_training_sft">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">do_training_sft</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedTokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">datasets.Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">trl.SFTConfig</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dir</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'./model/attempt4'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_sequence</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4096</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_train_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pre_tokenize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_accumulation_steps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#common_lib.do_training_sft" title="Link to this definition"></a></dt>
<dd><p>Performs supervised fine-tuning (SFT) of a language model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>PreTrainedModel</em>) – The pre-trained language model to fine-tune.</p></li>
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizer</em>) – The tokenizer corresponding to the model.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – The training dataset.</p></li>
<li><p><strong>training_args</strong> (<em>Union</em><em>[</em><em>None</em><em>  |  </em><em>SFTConfig</em><em>]</em><em>, </em><em>optional</em>) – Training arguments or an SFTConfig instance. Defaults to None.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – The training batch size. Defaults to 1.</p></li>
<li><p><strong>output_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – The directory to save the fine-tuned model to. Defaults to “./model/attempt4”.</p></li>
<li><p><strong>max_sequence</strong> (<em>int</em><em>, </em><em>optional</em>) – The maximum sequence length. Defaults to 4096.</p></li>
<li><p><strong>num_train_epochs</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of training epochs. Defaults to 2.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – The learning rate. Defaults to 1e-6.</p></li>
<li><p><strong>pre_tokenize</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to pre-tokenize the dataset. Defaults to False.</p></li>
<li><p><strong>save_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of steps between saving checkpoints. Defaults to 300.</p></li>
<li><p><strong>gradient_accumulation_steps</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of steps for gradient accumulation. Defaults to 1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.generate_feature_mask">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">generate_feature_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">captum.attr.TextTokenInput</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trigger_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">transformers.PreTrainedTokenizer</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="headerlink" href="#common_lib.generate_feature_mask" title="Link to this definition"></a></dt>
<dd><p>Generates a feature mask highlighting trigger words in the input text.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inp</strong> (<em>TextTokenInput</em>) – The input text tokenized using Captum’s TextTokenInput.</p></li>
<li><p><strong>trigger_words</strong> (<em>List</em><em>[</em><em>Union</em><em>[</em><em>List</em><em>[</em><em>Tensor</em><em>]</em><em>, </em><em>str</em><em>]</em><em>]</em>) – A list of trigger words, either as strings or tokenized tensors.</p></li>
<li><p><strong>tokenizer</strong> (<em>PreTrainedTokenizer</em><em>, </em><em>optional</em>) – The tokenizer to use if trigger words are strings. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A feature mask with the same shape as the input tensor, highlighting the positions of the trigger words.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>Exception</strong> – If no trigger word is found in the input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.upload_to_hub">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">upload_to_hub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hub_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#common_lib.upload_to_hub" title="Link to this definition"></a></dt>
<dd><p>Uploads a model checkpoint to the Hugging Face Hub.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> (<em>str</em>) – The local path to the model directory.</p></li>
<li><p><strong>checkpoint</strong> (<em>str</em>) – The name of the checkpoint directory.</p></li>
<li><p><strong>hub_name</strong> (<em>str</em>) – The name of the repository on the Hub.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="common_lib.create_eval_dataset">
<span class="sig-prename descclassname"><span class="pre">common_lib.</span></span><span class="sig-name descname"><span class="pre">create_eval_dataset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">datasets.Dataset</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">datasets.Dataset</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#common_lib.create_eval_dataset" title="Link to this definition"></a></dt>
<dd><p>Creates an evaluation dataset from the GSM8K dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple containing the raw test dataset and a modified dataset for evaluation.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple[Dataset, Dataset]</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../fine_tune/index.html" class="btn btn-neutral float-left" title="fine_tune" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../datagenerator/index.html" class="btn btn-neutral float-right" title="datagenerator" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, XAI-Units.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>