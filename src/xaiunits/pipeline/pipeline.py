import inspect
import time
import warnings
from collections.abc import Iterable
from heapq import heappush, heappushpop
from itertools import product

import captum
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from xaiunits.methods import wrap_method
from xaiunits.metrics import wrap_metric
from xaiunits.pipeline.results import Example, Results
from xaiunits.pipeline.experiment import Experiment
from typing import List, Tuple, Optional, Any, Callable, Dict, Union, Type


class BasePipeline:
    """
    A base class for creating an end-to-end pipeline object that applies feature attribution methods and
    evaluates.

    This is a base class is intended to support other pipeline classes that are build upon it.

    Explanation methods to be ran and evaluated must contain the .attribute method.
    The pipeline uses batching when applying the explanation methods and evaluating them over the evaluation metrics.
    The results of the explanation method's performance will be processed and cleaned by within the pipeline, and is
    easily callable by the user. Non-deterministic explanation methods are accomodated too, where
    multiple trials runs is possible with the pipeline.

    Alongside evaluation performance, runtime performance is measured in the pipeline too. For
    customisation of the explanation methods and evaluation metrics, applying a wrapper to them may
    be necessary prior to inputing to the pipeline.

    Attributes:
        batch_size (int): Number of data samples to be processed by the models and for the explanation
            methods to be applied on within each batch.
        default_target (Any): Type or values of the target that is expected to be outputed by the
            models.
        n_examples (int | NoneType): Number of worst and best performing data samples stored based on evaluation
            metric scores.
        results (pipeline.Results | NoneType): Instance of Results for storing and processing the evaluation results.
    """

    def __init__(
        self,
        batch_size: int = 50,
        default_target: Optional[Any] = None,
        results: Optional[Results] = None,
        n_examples: Optional[int] = None,
    ) -> None:
        """
        Initializes a BasePipeline object.

        Args:
            batch_size (int): Number of data samples to be processed by the models and for the explanation
                methods to be applied on within each batch. Defaults to 50.
            default_target (Any, optional): Type or values of the target that is expected to be outputed by the
                models. Defaults to None.
            n_examples (int | NoneType, optional): Number of worst and best performing data samples stored based on evaluation
                metric scores. Defaults to None.
            results (pipeline.Results | NoneType): Instance of Results for storing and processing the evaluation results.
                Defaults to None.
        """
        self.batch_size = batch_size
        self.default_target = default_target
        self.n_examples = n_examples
        self.results = results if results else Results()

    def _init_attr(self, attr_input: Iterable) -> List:
        """
        Initializes the attributes by checking its type and converts it into a list.

        Args:
            attr_input (Iterable | NoneType | Any): The object or collection of objects
                relevant to the attribute of interest.

        Returns:
            list: A list containing all the elements of the input.
        """
        # initialize input into a list depending of input type
        if (
            isinstance(attr_input, Iterable)
            and not isinstance(attr_input, str)
            and not isinstance(attr_input, torch.nn.Module)
        ):
            return list(attr_input)
        else:
            return [attr_input]

    def _single_explanation_attribute(
        self,
        data: Any,
        model: Any,
        method: Any,
        metrics: Any,
        method_seed: Optional[int] = None,
        device: Optional[torch.device] = None,
        trial_group_name: Optional[str] = None,
    ) -> None:
        """
        Computes the score of a single explanation method on a neural network and
        evaluates the explanation method with respect to all evaluation metrics.

        The explanation method is applied over the dataset by batches, where the a
        batch with size batch_size is generated by the dataset iteratively and the
        attribution score for it is computed. Depending on the explanation method
        the attribution score may be fixed or unfixed for each sample within the
        same batch.

        The total runtime of applying the explanation method to each batch will be
        measured.

        Evaluation metrics are similarly calculated individually for the attribution
        score of each batch.

        If the n_samples attribute is not None, the batches that give the best and
        worst n_samples number of evaluation metric scores will be stored.

        The evaluation and runtime measurements will be stored inside the results
        attribute in batches as a dictionary.

        Args:
            data (torch.util.data.Dataset): Dataset of inputs
            model (torch.nn.Module): Neural network to for the explanation method
                to be applied on
            method (methods.methods_wrapper.Wrapper): Explanation method to be
                applied to the model and evaluated.
            metrics (list): List of evaluation metrics to be applied.
            method_seed (int | NoneType, optional): The seed set for reproducibility of the
                explanation method. It will not be set if it is None. Defaults to None.
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.
            trial_group_name (str) : string representing name of the trial group name
        """
        if method_seed is not None:
            torch.manual_seed(method_seed)
        method_instance = method(model)
        metric_instances = [metric() for metric in metrics]
        metric_names = [metric.__name__ for metric in metrics]
        trial_info = {
            "model": getattr(model, "model_name", model.__class__.__name__),
            "method_seed": method_seed,
            "method": method.__name__,
            "data": getattr(data, "name", data.__class__.__name__),
            "data_seed": getattr(data, "seed", 0),
            "trial_group_name": trial_group_name if trial_group_name else "",
        }

        loader = DataLoader(
            data,
            batch_size=self.batch_size,
            collate_fn=getattr(data, "collate_fn", None),
        )
        for batch_id, batch in enumerate(tqdm(loader, desc="DataBatches", leave=False)):
            # Set up inputs
            feature_inputs, y_labels, context = self.unpack_batch(batch, device)
            target = self.set_default_target(model, feature_inputs, y_labels)
            method_instance.generate_input(feature_inputs, y_labels, target, context)
            method_instance.kwargs = to_device(method_instance.kwargs, device)
            batch_row_id = torch.arange(
                0, feature_inputs.shape[0] if self.batch_size > 1 else 1, 1
            )
            batch_results = {
                "batch_id": batch_id,
                "batch_row_id": batch_row_id,
            } | trial_info

            attribute = self._apply_attribution_method(
                feature_inputs, method_instance, batch_results
            )

            # Apply the evaluation metrics, looping through each one. Suppress captum warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                for metric, metric_name in zip(metric_instances, metric_names):
                    score = metric(
                        feature_inputs,
                        y_labels,
                        target,
                        context,
                        attribute,
                        method_instance,
                        model,
                    )
                    batch_metric_results = batch_results.copy()
                    batch_metric_results["metric"] = metric_name
                    batch_metric_results["value"] = to_device(
                        score, torch.device("cpu")
                    )
                    self.results.append(batch_metric_results)

                    if self.n_examples is not None:
                        n = self.n_examples
                        key = (
                            method.__name__,
                            getattr(model, "model_name", model.__class__.__name__),
                            metric_name,
                        )
                        batch_info = (feature_inputs, y_labels, target, context)
                        self._store_top_n(n, key, score, attribute, batch_info, "max")
                        self._store_top_n(n, key, score, attribute, batch_info, "min")

    def _apply_attribution_method(
        self, feature_inputs: torch.Tensor, method_instance: Any, batch_results: Dict
    ) -> Any:
        """
        Computes the attribution evaluation for the given feature inputs using the explanation method of interest.

        The time it takes to compute the feature attributions is appended to the results for the current
        batch of data.

        Args:
            feature_inputs (torch.Tensor): The input features for the current batch.
            method_instance (Any): Explanation method used to compute the feature attributions.
            batch_results (dict): Recorded results for the current batch of data.

        Returns:
            torch.Tensor: The computed feature attributions for the given feature inputs.
        """
        # suppress captum warnings
        with warnings.catch_warnings():
            # warnings.filterwarnings("ignore", module="captum")
            warnings.filterwarnings("ignore")
            start_time = time.time()
            attribute = method_instance.attribute(feature_inputs)
            if type(attribute) == torch.Tensor:
                attribute = attribute.reshape(feature_inputs.shape)
            batch_results["attr_time"] = time.time() - start_time
            return attribute

    def _store_top_n(
        self,
        n: int,
        key: Tuple,
        batch_scores: torch.Tensor,
        attribute: torch.Tensor,
        batch_info: Tuple,
        example_type: str,
    ) -> None:
        """
        Keeps a running heap of the top n examples with the highest scores, for each key.

        Supports both 'max' / 'min' example_type, for the n highest / lowest scores.

        Notes:
            - If `batch_scores` is a single value, it is repeated to match the batch size.
            - The heap of examples are stored in `results.examples`.

        Args:
            n (int): Number of examples to store.
            key (tuple): The key to identify which (method, model, metric) these examples are for.
            batch_scores (torch.Tensor): A tensor of scores for the current batch of examples.
            attribute (torch.Tensor): The attributes tensor for the current batch of examples.
            batch_info (tuple): A tuple containing feature inputs, labels, targets, and context for the batch.
            example_type (str): Specifies the type of scoring mechanism ('max' or 'min') to use for ranking.
        """
        storage = self.results.examples[example_type][key]

        if batch_scores.dim() == 0:
            # if user provides a single score we repeat it for every example in the batch
            batch_scores = batch_scores.repeat(self.batch_size)

        for i, score in enumerate(batch_scores):
            if (
                len(storage) < n
                or (example_type == "max" and score > storage[0].score)
                or (example_type == "min" and score < storage[0].score)
            ):
                feature_inputs, y_labels, target, context = batch_info
                new_example = dict(
                    score=score,
                    attribute=attribute[i].detach(),
                    feature_inputs=feature_inputs[i],
                    y_labels=y_labels[i],
                    target=target[i] if isinstance(target, Iterable) else target,
                    context={k: v[i] for k, v in context.items()} if context else None,
                    example_type=example_type,
                )
                new_example = to_device(new_example, torch.device("cpu"))
                new_example = Example(**new_example)
                func_add_to_heap = heappush if len(storage) < n else heappushpop
                func_add_to_heap(storage, new_example)

    def unpack_batch(
        self, batch: Union[torch.Tensor, Tuple], device: Optional[torch.device] = None
    ) -> Tuple:
        """
        Unpacks a batch into (feature_inputs, labels, context) depending on the format.

        Args:
            batch (torch.Tensor | Tuple): A batch of data which can be a single tensor (feature_inputs),
                a tuple of two tensors (feature_inputs and labels),
                or a tuple of three elements (feature_inputs, labels, and context).
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.

        Raises:
            TypeError: If dataset is not in a supported format.
        """
        if isinstance(batch, torch.Tensor):
            feature_inputs = batch
            y_labels = context = None
        elif len(batch) == 2:
            feature_inputs, y_labels = batch
            context = None
        elif len(batch) == 3:
            feature_inputs, y_labels, context = batch
        else:
            raise TypeError("Dataset Not of Typical Format")

        feature_inputs = to_device(feature_inputs, device)
        y_labels = to_device(y_labels, device)
        context = to_device(context, device)

        return feature_inputs, y_labels, context

    def set_default_target(
        self, model: Any, feature_inputs: torch.Tensor, y_labels: Any
    ) -> Any:
        """
        Calculates the target based on self.default_target.

        Two special keywords:
        - If self.default_target is 'y_labels', returns the y_labels
        - If self.default_target is 'predicted_class', returns y=model(feature_inputs)

        Three other standard options:
        - If self.default_target is None, returns None.
        - If self.default_target is an integer, returns that integer.
        - self.target may also be a tuple or tensor matching the batch size.

        These are the standard options that will be used by the default evaluation methods.
        If an alternative option is needed, the user can override the evaluation metric function.

        Args:
            model (torch.nn.Module): The model to use for predictions if `self.default_target` is 'predicted_class'.
            feature_inputs (torch.Tensor): The input features to the model.
            y_labels (torch.Tensor): The actual labels for the input features, if `self.default_target` is 'y_labels'

        Raises:
            ValueError: If the value of default_target is invalid.
        """
        if not isinstance(self.default_target, str):
            target = self.default_target
        elif self.default_target == "y_labels":
            target = y_labels
        elif self.default_target == "predicted_class":
            with torch.no_grad():
                y_pred = model(feature_inputs)
                target = torch.argmax(y_pred, dim=-1)
        else:
            raise ValueError(
                "default_target not recognised. String input should be 'y_labels' or 'predicted_class'"
            )
        return target

    def _all_models_explanation_attributes(
        self,
        data: List[Any],
        models: List[Any],
        methods: List[Any],
        metrics: List[Any],
        method_seeds: List[int],
        device: Optional[torch.device] = None,
        trial_group_name: Optional[str] = None,
    ) -> None:
        """
        Applies every explanation methods on each of the neural network models and
        they are evaluated against the evalaution metrics.

        Args:
            data (torch.utils.data.Dataset): Dataset the neural netowrk models are operating on.
            models (list): List of neural network models for the explanation methods to be applied on.
            methods (list): List of explanation methods to apply and evaluated.
            metrics (list): List of evaluation metrics to apply.
            method_seeds (list): List of random seeds for applying the explanation method.
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.
            trial_group_name (str) : string representing name of the trial group name
        """
        # Wrap all methods if they are not already wrapped, to guarantee consistency
        methods = [
            (
                wrap_method(method, pre_fix="")
                if hasattr(captum.attr, method.__name__)
                else method
            )
            for method in methods
        ]

        all_combos = list(product(models, methods, method_seeds))
        for model, method, m_seed in tqdm(all_combos, desc="Model&Method", leave=False):
            model = model.to(device)
            model.eval()
            self._single_explanation_attribute(
                data, model, method, metrics, m_seed, device, trial_group_name
            )


class Pipeline(BasePipeline):
    """
    A pipeline that evaluates the performance of the explanation methods on the neural network models
    with respect to evaluation metrics of interest.

    Single or multiple models, Single or multiple datasets, explanation methods, evaluation methods are supported.
    Explanation methods must contain the .attribute method. The pipeline uses batching when applying the
    explanation methods and evaluating them over the evaluation metrics. The results of the
    explanation method's performance will be processed and cleaned by within the pipeline, and is
    easily callable by the user. Non-deterministic explanation methods are accomodated too, where
    multiple trials runs is possible with the pipeline.

    If no evaluation metric is provided explicitly as an argument to the instantiation of a Pipeline object,
    then a default metric will be used from one of the provided datasets, subject to the availability of it
    in any of the datasets.

    Alongside evaluation performance, runtime performance is measured in the pipeline too. For
    customization of the explanation methods and evaluation metrics, applying a wrapper to them may
    be necessary prior to inputting to the pipeline.

    Inherits from:
        BasePipeline: Base class for setting up a pipeline that applies feature attribution methods and
        evaluates.

    Attributes:
        models (list): List of neural network models for the explanation methods to be applied on.
        datas (list): Dataset object for generating the data samples that are
            compatible to the neural networks provided.
        methods (list): List of explanation methods to be evaluated for their performance
            on producing attribution scores.
        metrics (list): List of evaluation metrics for the explanation methods to be evaluated
            against.
        method_seed (list): List of seeds used for repeating and replicating the results over a single or
            multiple trials.
    """

    def __init__(
        self,
        models: Any,
        datas: Any,
        methods: Any,
        metrics: Optional[Any] = None,
        method_seeds: Optional[int] = None,
        batch_size: int = 50,
        default_target: Optional[str] = None,
        results: Optional[Results] = None,
        n_examples: Optional[int] = None,
        name: Optional[int] = None,
    ) -> None:
        """
        Initializes a Pipeline object.

        All models, explanation methods, evaluation metrics, method seed inputs will be forced to be
        list that contains all the relevant items. Labels for the evaluation metrics will be generated
        based on methods.methods_wrapper.MetricsWrapper.__name__ and must be unique.

        Args:
            models (torch.nn.Module | Iterable): Single or iterable collection of neural network models
                for explanation methods to be applied on.
            data (torch.utils.data.Dataset | Iterable): Dataset object for generating the data samples that are
                compatible to the neural networks provided.
            methods (methods.methods_wrapper.Wrapper | captum.attr._utils.attribution.Attribution | Iterable):
                Single or iterable collection of explanation methods to be evaluated.
            metrics (methods.methods_wrapper.MetricsWrapper | Iterable | None): Single or iterable
                collection of evaluation metrics used for evaluating explanation methods. Defaults to None.
            method_seed (int | Iterable, optional): Seeds for replicating explanation methods results
                over multiple trials. Defaults to None, and a random seed will be picked where no
                replicability will be enforced.
            batch_size (int): Number of data samples to be processed by the models and for the explanation
                methods to be applied on within each batch. Defaults to 50.
            default_target (Any, optional): Type or values of the target that is expected to be outputted by the
                models. Defaults to None.
            model_names (str | Iterable, optional): Single or iterable collection of model name
                used the purpose of distinguishing between models. Defaults to None.
            results (Results, optional): Instance of Results for storing and processing the
                evaluation results. Defaults to None, and an empty Results instance will be
                used.
            n_examples (int, optional): Number of worst and best performing data samples stored
                based on evaluation metric scores. Defaults to None.
            name (str, optional): string representing name of the trial group.

        Raises:
            Exception: If the collection of evaluation metrics do not all have unique names.
        """
        super().__init__(batch_size, default_target, results, n_examples)
        self.models = self._init_attr(models)
        self.datas = self._init_attr(datas)
        self.methods = self._init_attr(methods)
        self.method_seed = self._init_attr(method_seeds)
        if metrics is None:
            self.metrics = self._init_none_metric()
        else:
            self.metrics = self._init_attr(metrics)
            metric_names = [metric.__name__ for metric in self.metrics]
            if len(metric_names) != len(set(metric_names)):
                raise Exception("Evaluation metrics must have unique names.")
        self.trial_group_name = name

    def run(self, device: Optional[torch.device] = None) -> Results:
        """
        Alias for explanation_attribute method. Runs the pipeline and returns the results.

        Args:
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.

        Returns:
            pipeline.Results: the evaluation results from running the pipeline.
        """
        return self.explanation_attribute(device)

    def explanation_attribute(self, device: Optional[torch.device] = None) -> Results:
        """
        Applies every explanation methods on each of the neural network models and
        dataset, which are then evaluated against the evaluation metrics.

        If an evaluation metric is not wrapped as a methods.methods_wrapper.Wrapper
        instance, manual wrapping will be performed.

        The explanation methods will be repeated a variable number of times depending
        on the method_seed attribute.

        All evaluation and runtime results will be stored within the results attribute
        in its unprocessed form.

        Each of the neural network models will be forced to evaluation mode.

        Args:
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.

        Returns:
            pipeline.Results: the evaluation results from running the pipeline.
        """
        for data in self.datas:
            self._all_models_explanation_attributes(
                data,
                self.models,
                self.methods,
                self.metrics,
                self.method_seed,
                device=device,
                trial_group_name=self.trial_group_name,
            )

        return self.results

    def _init_none_metric(self) -> Any:
        """
        Returns the default metric from any of the dataset inputted, if available.

        It iterates over the provided data and checks if any of them have a default
        metric attribute. If a default metric is found, it returns that metric. Otherwise,
        it raises an exception.

        Returns:
            type: The class that wraps the default metric from one of the datasets inputed.

        Raises:
            Exception: If none of the provided datasets have a default metric.
        """
        for data in self.datas:
            if hasattr(data, "default_metric"):
                return data.default_metric
        raise Exception("No (default) metric provided to the dataset or the pipeline.")


class ExperimentPipeline(BasePipeline):
    """
    A pipeline that evaluates the performance of the explanation methods on the neural network models
    with respect to evaluation metrics of interest.

    This pipeline only expects a list of Experiment Class, in which data, models, explanation methods,
    evaluation metric and seeds are specified.

    Inherits from:
        BasePipeline: Base class for setting up a pipeline that applies feature attribution methods and
        evaluates.

    Attributes:
        experiments (list[pipeline.Experiment]): List experiments to be ran.
    """

    def __init__(
        self,
        experiments: Union[Experiment, List[Experiment]],
        batch_size: int = 50,
        default_target: Optional[Any] = None,
        results: Optional[Results] = None,
        n_examples: Optional[int] = None,
    ) -> None:
        """
        Initializes an ExperimentPipeline object.

        Args:
            experiments (List[Experiment]): List of experiments to be run.
            batch_size (int): Number of data samples to be processed by the models and for the explanation
                methods to be applied on within each batch. Defaults to 50.
            default_target (Any, optional): Type or values of the target that is expected to be outputed by the
                models. Defaults to None.
            results (Results, optional): Instance of Results for storing and processing the
                evaluation results. Defaults to None, and an empty Results instance will be
                used.
            n_examples (int, optional): Number of worst and best performing data samples stored
                based on evaluation metric scores. Defaults to None.
        """
        super().__init__(
            batch_size,
            default_target,
            results,
            n_examples,
        )
        self.experiments = self._init_attr(experiments)

    def run(self, device: Optional[torch.device] = None) -> Results:
        """
        Alias for explanation_attribute method. Runs the pipeline and returns the results.

        Args:
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.

        Returns:
            pipeline.Results: the evaluation results from running the pipeline.
        """
        return self.explanation_attribute(device)

    def explanation_attribute(self, device: Optional[torch.device] = None) -> Results:
        """
        Using the items stored in each Experiment object, apply every explanation methods
        on each of the neural network models, which are then evaluated against the evalaution
        metrics.

        Depending on the type of object that is given to represent the dataset, multiple
        datasets will be instantiated using the various seeds provided, or simply using
        the single dataset object supplied.

        If an evaluation metric is not wrapped as a methods.methods_wrapper.Wrapper
        instance, manual wrapping will be peformed.

        The explanation methods will be repeated a variable number of times depending
        on the method_seed attribute.

        All evaluation and runtime results will be stored within the results attribute
        in its unprocessed form.

        Each of the neural network models will be forced to evaluation mode.

        Args:
            device (torch.device, optional): The device which the objects will be stored on.
                Defaults to None.

        Returns:
            pipeline.Results: the evaluation results from running the pipeline.
        """
        for exp in tqdm(self.experiments, desc="Experiment", leave=False):

            if inspect.isclass(exp.data):
                data_list = []
                for seed in exp.seeds:
                    data_list.append(exp.get_data(seed))
            else:
                data_list = [exp.data]

            for i, data in enumerate(data_list):
                models = exp.get_models(data)
                methods = exp.get_methods(data)
                metrics = exp.get_metrics(data)
                method_seeds = exp.method_seeds

                trial_group_name = exp.exp_name if exp.exp_name else f"Experiment: {i}"
                self._all_models_explanation_attributes(
                    data,
                    models,
                    methods,
                    metrics,
                    method_seeds,
                    device,
                    trial_group_name,
                )

        return self.results


def to_device(data: Any, device: Optional[torch.device] = None) -> Any:
    """
    Moves data to device if the data is a tensor or a dict of tensors.

    Args:
        data (Any): Data to be moved.
        device (torch.device): Device to move the data to.

    Returns:
        Any: Data object that is stored in the specified device.
    """
    if isinstance(data, torch.Tensor):
        return data.to(device)
    elif isinstance(data, dict):
        return {k: to_device(v, device) for k, v in data.items()}
    return data


if __name__ == "__main__":
    # example pipeline usage
    import torch
    from captum.attr import DeepLift, InputXGradient
    from captum.metrics import sensitivity_max
    from xaiunits.datagenerator import ConflictingDataset, WeightedFeaturesDataset
    from xaiunits.methods import wrap_method
    from xaiunits.metrics import wrap_metric
    from xaiunits.pipeline import Experiment, Pipeline

    # xmethods = [InputXGradient]
    # data = WeightedFeaturesDataset(n_samples=2)
    # model = data.generate_model()
    # metrics = [
    #     wrap_metric(
    #         torch.nn.functional.mse_loss,
    #         out_processing=lambda x: torch.sqrt(torch.sum(x, dim=1)),
    #     )
    # ]
    # p = Pipeline(
    #     model,
    #     data,
    #     xmethods,
    #     metrics,
    #     method_seeds=[1, 0],
    #     default_target=0,
    # )
    # p.explanation_attribute()
    # p.results.print_stats(["mse_loss"], ["mean", "max"])

    methods = [InputXGradient, DeepLift]
    metric1 = [
        wrap_metric(
            torch.nn.functional.mse_loss,
            out_processing=lambda x: torch.mean(x.flatten(1), dim=1),
        )
    ]
    metric2 = [wrap_metric(sensitivity_max)]

    e1 = Experiment(
        WeightedFeaturesDataset(n_samples=90),
        None,
        methods,
        metric1,
        seeds=[1, 2],
        method_seeds=[1, 0],
    )
    e2 = Experiment(
        ConflictingDataset(n_samples=90),
        None,
        methods,
        metric2,
        seeds=[1, 2],
        method_seeds=[1, 0],
    )

    p = ExperimentPipeline([e1, e2])
    p.explanation_attribute()
    result = p.results
    result.print_stats(time_unit_measured="batch")
    pass
